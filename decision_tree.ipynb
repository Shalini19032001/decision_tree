{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# theoretical\n"
      ],
      "metadata": {
        "id": "Z1MSTH9cWgws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 1.What is a Decision Tree, and how does it work\t.\n",
        " - A Decision Tree is a flowchart-like structure used for classification and regression tasks. It splits the dataset into subsets based on the value of input features. Each node represents a feature, each branch a decision rule, and each leaf a result."
      ],
      "metadata": {
        "id": "2PX2tHNyWhIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are impurity measures in Decision Trees\n",
        "- Impurity measures quantify the disorder or uncertainty in data. Common measures are Gini Impurity and Entropy, used to decide how to split the data at each node."
      ],
      "metadata": {
        "id": "9IsdcwC3WhTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the mathematical formula for Gini Impurity\n",
        "- Gini=1−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "2\n",
        "​\n"
      ],
      "metadata": {
        "id": "jvlFx3wFWhbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the mathematical formula for Entropy\n",
        "- Entropy=−\n",
        "i=1\n",
        "∑\n",
        "n\n",
        "​\n",
        " p\n",
        "i\n",
        "​\n",
        " log\n",
        "2\n",
        "​\n",
        " (p\n",
        "i\n",
        "​\n",
        " )"
      ],
      "metadata": {
        "id": "SZF8Hh3OWhgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  What is Information Gain, and how is it used in Decision Trees\n",
        "- Information Gain is the reduction in entropy after a dataset is split on an attribute. It's used to select the feature that best splits the data:\n",
        "\n",
        "- 𝐼\n",
        "𝑛\n",
        "𝑓\n",
        "𝑜\n",
        "𝑟\n",
        "𝑚\n",
        "𝑎\n",
        "𝑡\n",
        "𝑖\n",
        "𝑜\n",
        "𝑛\n",
        "\n",
        "𝐺\n",
        "𝑎\n",
        "𝑖\n",
        "𝑛\n",
        "=\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        ")\n",
        "−\n",
        "∑\n",
        "𝑖\n",
        "∣\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        "∣\n",
        "∣\n",
        "𝑝\n",
        "𝑎\n",
        "𝑟\n",
        "𝑒\n",
        "𝑛\n",
        "𝑡\n",
        "∣\n",
        "×\n",
        "𝐸\n",
        "𝑛\n",
        "𝑡\n",
        "𝑟\n",
        "𝑜\n",
        "𝑝\n",
        "𝑦\n",
        "(\n",
        "𝑐\n",
        "ℎ\n",
        "𝑖\n",
        "𝑙\n",
        "𝑑\n",
        "𝑖\n",
        ")\n",
        "- Information Gain=Entropy(parent)−\n",
        "i\n",
        "∑\n",
        "​\n",
        "  \n",
        "∣parent∣\n",
        "∣child\n",
        "i\n",
        "​\n",
        " ∣\n",
        "​\n",
        " ×Entropy(child\n",
        "i\n",
        "​\n",
        " )"
      ],
      "metadata": {
        "id": "cXrcDteKWhjk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the difference between Gini Impurity and Entropy\n",
        "- Both measure impurity, but Gini is faster to compute. Entropy tends to give slightly more balanced trees but is computationally heavier."
      ],
      "metadata": {
        "id": "Spt8uHQUWhm2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is the mathematical explanation behind Decision Trees\n",
        "- Decision Trees use a recursive algorithm (like ID3, CART) to split nodes based on criteria like Gini or Entropy to minimize impurity and build a tree from top to bottom."
      ],
      "metadata": {
        "id": "2O2h9pnhWhpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is Pre-Pruning in Decision Trees\n",
        "- Pre-pruning stops tree growth early by setting limits (e.g., max_depth, min_samples_split) to prevent overfitting."
      ],
      "metadata": {
        "id": "FePPit0eWhsl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  What is Post-Pruning in Decision Trees\n",
        "- Post-pruning allows full tree growth, then prunes unnecessary nodes based on validation set performance to improve generalization."
      ],
      "metadata": {
        "id": "NDt15V0gWhu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  What is the difference between Pre-Pruning and Post-Pruning\n",
        "- Pre-pruning prevents growth during training.\n",
        "\n",
        "- Post-pruning removes branches after training."
      ],
      "metadata": {
        "id": "sM4N-v1qWhyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11.  What is a Decision Tree Regressor\n",
        "- A Decision Tree Regressor is a variant used for regression tasks, where the output is a continuous value."
      ],
      "metadata": {
        "id": "uUc8u7s9Wh1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What are the advantages and disadvantages of Decision Trees\n",
        "- Advantages: Easy to interpret, handles non-linear data, requires little data preprocessing.\n",
        "- Disadvantages: Prone to overfitting, unstable with small data changes."
      ],
      "metadata": {
        "id": "LBh_7aLoWh4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13.  How does a Decision Tree handle missing values\n",
        "- By ignoring missing values, using surrogate splits, or imputing them before training."
      ],
      "metadata": {
        "id": "4y7higyXWh6Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How does a Decision Tree handle categorical features\n",
        "- Categorical features are split using equality-based rules (e.g., feature == value), or label encoding/one-hot encoding can be used."
      ],
      "metadata": {
        "id": "fe9W8fZ8Wh9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What are some real-world applications of Decision Trees?\n",
        "- Medical diagnosis, customer segmentation, credit scoring, fraud detection, etc."
      ],
      "metadata": {
        "id": "F5VjVrw4WiAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# practical"
      ],
      "metadata": {
        "id": "kOjuTumEWiDm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy\n",
        "- from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "- iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "- clf = DecisionTreeClassifier()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "- y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "kNVMHtozWiHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances\n",
        "- clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e739IhjgWiKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the\n",
        "model accuracy\n",
        "- clf = DecisionTreeClassifier(criterion='entropy')\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred = clf.predict(X_test)\n",
        "print(\"Accuracy (Entropy):\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "5806OlYAWiN_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean\n",
        "Squared Error (MSE)\n",
        "- from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "- data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "- reg = DecisionTreeRegressor()\n",
        "reg.fit(X_train, y_train)\n",
        "y_pred = reg.predict(X_test)\n",
        "print(\"MSE:\", mean_squared_error(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "ILeMPUBoWiRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz\n",
        "- from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "- dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph.render(\"decision_tree\")  # Saves as decision_tree.pdf\n"
      ],
      "metadata": {
        "id": "9XKskTqaWiUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its\n",
        "accuracy with a fully grown tree\n",
        "- clf_full = DecisionTreeClassifier()\n",
        "clf_limited = DecisionTreeClassifier(max_depth=3)\n",
        "\n",
        "- clf_full.fit(X_train, y_train)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "\n",
        "- print(\"Full Tree Accuracy:\", accuracy_score(y_test, clf_full.predict(X_test)))\n",
        "print(\"Limited Tree Accuracy:\", accuracy_score(y_test, clf_limited.predict(X_test)))\n",
        "\n"
      ],
      "metadata": {
        "id": "x7hTB7ECbFKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its\n",
        "accuracy with a default tree\n",
        "- clf_default = DecisionTreeClassifier()\n",
        "clf_modified = DecisionTreeClassifier(min_samples_split=5)\n",
        "\n",
        "- clf_default.fit(X_train, y_train)\n",
        "clf_modified.fit(X_train, y_train)\n",
        "\n",
        "- print(\"Default Tree Accuracy:\", accuracy_score(y_test, clf_default.predict(X_test)))\n",
        "- print(\"min_samples_split=5 Accuracy:\", accuracy_score(y_test, clf_modified.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "qW-8uVQIbFaX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23.  Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its\n",
        "accuracy with unscaled data\n",
        "- from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "- scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "- clf_scaled = DecisionTreeClassifier()\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "- print(\"Scaled Data Accuracy:\", accuracy_score(y_test, clf_scaled.predict(X_test_scaled)))\n",
        "- print(\"Unscaled Data Accuracy:\", accuracy_score(y_test, clf.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "uCwD5d-jbFp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass\n",
        "classification\n",
        "- from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "- ovr_clf = OneVsRestClassifier(DecisionTreeClassifier())\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "- print(\"OvR Accuracy:\", ovr_clf.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "4F1fVSAibFwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Decision Tree Classifier and display the feature importance scores\n",
        "- print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "id": "ClPhzJk2bFzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance\n",
        "with an unrestricted tree\n",
        "- reg_full = DecisionTreeRegressor()\n",
        "reg_limited = DecisionTreeRegressor(max_depth=5)\n",
        "\n",
        "- reg_full.fit(X_train, y_train)\n",
        "reg_limited.fit(X_train, y_train)\n",
        "\n",
        "- print(\"Full Tree MSE:\", mean_squared_error(y_test, reg_full.predict(X_test)))\n",
        "print(\"max_depth=5 MSE:\", mean_squared_error(y_test, reg_limited.predict(X_test)))\n"
      ],
      "metadata": {
        "id": "R5HBm3iCbF1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and\n",
        "visualize its effect on accuracy\n",
        "- path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas = path.ccp_alphas\n",
        "\n",
        "- for ccp_alpha in ccp_alphas:\n",
        "    clf_pruned = DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
        "    clf_pruned.fit(X_train, y_train)\n",
        "    print(f\"Alpha: {ccp_alpha:.4f}, Accuracy: {accuracy_score(y_test, clf_pruned.predict(X_test))}\")\n"
      ],
      "metadata": {
        "id": "Q3dlANJ7bF-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision,\n",
        "Recall, and F1-Score\n",
        "- from sklearn.metrics import classification_report\n",
        "\n",
        "- y_pred = clf.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "id": "zUIvXY9VbHXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn\n",
        "- import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "- cm = confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SVU28WGmdInW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30.  Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values\n",
        "for max_depth and min_samples_split.\n",
        "- from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "- param_grid = {'max_depth': [2, 3, 4, 5, None], 'min_samples_split': [2, 5, 10]}\n",
        "grid = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "- print(\"Best Parameters:\", grid.best_params_)\n",
        "- print(\"Best Score:\", grid.best_score_)\n"
      ],
      "metadata": {
        "id": "9bjYMm9EdIwW"
      }
    }
  ]
}